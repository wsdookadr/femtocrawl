== Intro

This is a minimalistic dynamic page crawler accompanied by a variety of
tools used to produce sitemaps and some tools to deal with WARC files.

== Quick start

Clone femtocrawl and pull the latest femtocrawl image:

----
git clone https://github.com/wsdookadr/femtocrawl
cd femtocrawl
docker pull wsdookadr/femtocrawl
----

Create the required directories:

----
mkdir input/ warc/ zim/
----

Populate the input url list:

----
echo -e "https://lobste.rs\nhttps://news.ycombinator.com\nhttp://google.com\nhttp://mozilla.org" > input/list_urls.txt
----

Run the crawl

----
./bin/op.py --crawl
----

After the previous step, you'll see the following files:

----
user@garage3:~/zim-bench/femtocrawl$ ls -l warc/
total 1088
-rw-r--r-- 1 user user      84 Aug 22 01:53 1.urls
-rw-r--r-- 1 user user 1109534 Aug 22 01:53 1.warc
----

At this point, you can check the contents of the WARC using link:https://replayweb.page/[replayweb.page]

Now it's time for validation

----
./bin/op.py --validate
----

If invalid WARCs are reported, you can investigate further or exclude them by deleting them.

When all WARCs in the `warc/` directory are valid, you can join all the WARCs into `warc/big.zim`

----
./bin/op.py --join
----

At this point you can convert to ZIM

----
./bin/op.py --zim
----

And you can serve the archive locally at http://localhost:8083 like this:

----
./bin/op.py --kiwix
----

You can also use multiple of these switches at the same time.

== Roadmap

* [x] basic crawling of web pages
* [x] warc joining
* [x] zim conversion
* [x] kiwix integration
* [x] warc text indexing for html and pdf records and search
* [ ] pdf autodownload (when the pdf does not load, but a dialog shows up instead)
* [ ] archive.org support for an entire website to be downloaded. one
      of the problems is fixing the old links which may be invalid and that may
      require patching the warc
      (target link:https://github.com/hartator/wayback-machine-downloader/issues[issues] found here )
* [ ] rewrite bin/femtocrawl.sh in python but retain logic, add args including
      har output switch
* [ ] add chromium support
* [ ] build capability to compare har files for the same web page loaded in
      different browsers.
      (request completion times, uris of the requests made, response status codes)
* [ ] investigate usage of the latest browser binaries, cut down on
      distribution package dependencies and decrease the docker image size
* [ ] generalized and easy to use sitemap generation

== FAQ

=== How does it work?

More details about the way it works are in link:https://wsdookadr.github.io/posts/p8/[this blog post].

=== My host user UID/GID don't match the container UID/GID. What can I do?

For now, just change them in the Dockerfile and rebuild the docker image.

=== I want to change the browser profile, add extensions or userscripts, how do I do that?

Run the following on the host to get the Firefox profile

----
id=$(docker create wsdookadr/femtocrawl:latest)
docker cp $id:/home/user/ff ~/.mozilla/firefox/p1
docker rm -v $id
----

Start Firefox on the host with `firefox --profile ~/.mozilla/firefox/p1`.
Make any changes you want to it, close Firefox, zip the profile and place it in `data/ff.zip`
and rebuild the Docker image.

NOTE: The default ff profile comes with 
link:https://violentmonkey.github.io/api/gm/[violentmonkey] and 
link:https://github.com/gorhill/uBlock[uBlock].

=== I want to crawl a site that requires me to log in

See the previous item

=== I have some sites I'd like to crawl, what do I do?

On the host, do the following: place the urls you want crawled in a file,
one per line and run `bin/triage_new_links.sh` on that file, that will
produce two files `with_sitemap.txt` and `without_sitemap.txt`. Now
add the contents of those to `bin/gen_sitemap.sh` and run it. This will
produce `list_urls2.txt` which you can use as input for femtocrawl.

=== I want to crawl some parts of reddit and read them offline, how do I do that?

Have a look at link:https://github.com/wsdookadr/femtocrawl/blob/27fed88f4b1f99bf7917b9eecab753610fe653ed/bin/sitemap_reddit.py[sitemap_reddit.py]

=== What kind of performance can I expect?

On a 56 Mbps connection with 10 urls and 29 seconds per batch, you can
crawl 29k urls per day. The CPU usage is minimal.

=== What do I use this for?

Use-cases:

* building offline web archives
* website testing
* cross-testing different web archiving tools
* long-term news archiving


